{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lista 3 - Sorts Eficientes - EDAII\n",
    "#### Alunos: Ícaro Pires (15/0129815) e Vinicius Bernardo (15/0151331)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Escolhemos trabalhar com dados reais, para que o número de elementos fosse relativamente alto e seus valores significativos. Os dados usadaos estão disponibilizados pelo governo federal em sua plataforma de dados abertos. No caso em questão, foram escolhidos os dados referente as ocorrências policiais no Brasil desde o ano de 2004 até 2017. (Com exceção de 2016, porque os dados estão indisponíveis no website).\n",
    "</p>\n",
    "\n",
    "## Solução\n",
    "\n",
    "<p>\n",
    "Foi escolhido a técnica de ordenação Bucket de maneira que o argumento de separação dos baldes escolhido foram a quantidade de ocorrências de cada registro, sendo um registro por tipo de crime, localidade e data. Como o número de ocorrências se repete em muitos registros, o algoritimo de ordenação Merge Sort foi o escolhido por sua eficiência e por ser estável, e em sua versão iterativa, pela linguagem (python) não dar muito suporte a recursão. Resumindo, o merge sort será aplicado em cada balde individualmente.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Além disso, para extrair o máximo de poder computacional e aproveitar da vantagem da separação em baldes, foi utilizado também a ferramenta para Data Science e processamento paralelo e distribuído: Apache Spark, ela é a responsável por paralelizar o Merge Sort em cada um dos baldes. O Spark por si só irá gerenciar a separação de jobs e sua subdivisão em tasks. A quantidade de tarefas executando em paralelo são 4, porque é a quantidade de núcleos que o computador usado nos testes possui.\n",
    "</p>\n",
    "\n",
    "O resumo da execução dos jobs e das tasks podem ser conferidos nas imagens (prints) disponíveis no repositório, para a execução utilizando-se apenas um núcleo e a execução usando todos os núcleos (4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desenvolvimento da solução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_police_reports_df = spark.read.csv('ocorrencias/ocorrencias_2017_2004.csv', sep=';', encoding='latin1', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando coluna da quantidade de ocorrências e vinculando índices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "police_reports_amount = all_police_reports_df.select('PC-Qtde Ocorrências').toPandas().astype(float).values\n",
    "indexes = range(len(police_reports_amount))\n",
    "police_reports_amount = list(zip(police_reports_amount, indexes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando Dados em 4 baldes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de elementos nos baldes:\n",
      "\tprimeiro: 115893,\n",
      "\tsegundo: 71547,\n",
      "\tterceiro: 54563,\n",
      "\tquarto: 66624,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A quantidade de baldes foi definida após algumas experimentações com diferentes valores\n",
    "buckets = [[], [], [], []]\n",
    "for element in police_reports_amount:\n",
    "    if element[0] < 2:\n",
    "        buckets[0].append(element)\n",
    "    elif element[0] >= 2 and element[0] < 4:\n",
    "        buckets[1].append(element)\n",
    "    elif element[0] >= 4 and element[0] < 10:\n",
    "        buckets[2].append(element)\n",
    "    else:\n",
    "        buckets[3].append(element)\n",
    "        \n",
    "print(f'Quantidade de elementos nos baldes:\\n'\n",
    "      f'\\tprimeiro: {len(buckets[0])},\\n'\n",
    "      f'\\tsegundo: {len(buckets[1])},\\n'\n",
    "      f'\\tterceiro: {len(buckets[2])},\\n'\n",
    "      f'\\tquarto: {len(buckets[3])},\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação do algoritmo de ordenação (Merge Sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeSort(x):\n",
    "    x = list(x)[0]\n",
    "    def merge(a, b):\n",
    "        x = []\n",
    "        while len(a) > 0 and len(b) > 0:\n",
    "            if a[0][0] < b[0][0]:\n",
    "                x.append(a.pop(0))\n",
    "            else:\n",
    "                x.append(b.pop(0))\n",
    "        while len(a) > 0:\n",
    "            x.append(a.pop(0))\n",
    "        while len(b) > 0:\n",
    "            x.append(b.pop(0))\n",
    "        return x\n",
    "        \n",
    "    queue = [[i] for i in x]\n",
    "    while len( queue ) > 1:\n",
    "        a, b = queue.pop(0), queue.pop(0)\n",
    "        queue.append(merge(a, b))\n",
    "    yield queue[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando o MergeSort paralelamente em cada um dos baldes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets_results = sc.parallelize(buckets, len(buckets)).mapPartitions(mergeSort).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exibição dos resultados de cada balde individualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementos nos baldes:\n",
      "\tPrimeiro balde:\n",
      "\tPrimeiros 3 elementos: [(array([1.]), 276518), (array([1.]), 276517), (array([1.]), 276516)]\n",
      "\tÚltimos 3 elementos: [(array([1.974]), 17121), (array([1.99]), 1604), (array([1.996]), 1598)]\n",
      "\n",
      "\tSegundo balde:\n",
      "\tPrimeiros 3 elementos: [(array([2.]), 64199), (array([2.]), 64198), (array([2.]), 64190)]\n",
      "\tÚltimos 3 elementos: [(array([3.983]), 4627), (array([3.988]), 88616), (array([3.99]), 66122)]\n",
      "\n",
      "\tTerceiro balde:\n",
      "\tPrimeiros 3 elementos: [(array([4.]), 234831), (array([4.]), 234826), (array([4.]), 234822)]\n",
      "\tÚltimos 3 elementos: [(array([9.]), 235331), (array([9.]), 235322), (array([9.]), 234848)]\n",
      "\n",
      "\tQuarto balde:\n",
      "\tPrimeiros 3 elementos: [(array([10.]), 5027), (array([10.]), 5024), (array([10.]), 5023)]\n",
      "\tÚltimos 3 elementos: [(array([996.]), 55138), (array([998.]), 101632), (array([998.]), 89038)]\n"
     ]
    }
   ],
   "source": [
    "# Não são exibidos todos os dados porque são mais de 300 mil\n",
    "n = 3\n",
    "print(f'Elementos nos baldes:\\n'\n",
    "      f'\\tPrimeiro balde:\\n'\n",
    "      f'\\tPrimeiros {n} elementos: {buckets_results[0][:n]}\\n'\n",
    "      f'\\tÚltimos {n} elementos: {buckets_results[0][-n:]}\\n\\n'\n",
    "      \n",
    "      f'\\tSegundo balde:\\n'\n",
    "      f'\\tPrimeiros {n} elementos: {buckets_results[1][:n]}\\n'\n",
    "      f'\\tÚltimos {n} elementos: {buckets_results[1][-n:]}\\n\\n'\n",
    "      \n",
    "      f'\\tTerceiro balde:\\n'\n",
    "      f'\\tPrimeiros {n} elementos: {buckets_results[2][:n]}\\n'\n",
    "      f'\\tÚltimos {n} elementos: {buckets_results[2][-n:]}\\n\\n'\n",
    "      \n",
    "      f'\\tQuarto balde:\\n'\n",
    "      f'\\tPrimeiros {n} elementos: {buckets_results[3][:n]}\\n'\n",
    "      f'\\tÚltimos {n} elementos: {buckets_results[3][-n:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unindo conteúdo dos baldes em um resultado final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiros 3 elementos da lista:\n",
      "[(array([1.]), 276518), (array([1.]), 276517), (array([1.]), 276516)]\n",
      "\n",
      "Útimos 3 elementos da lista:\n",
      "[(array([996.]), 55138), (array([998.]), 101632), (array([998.]), 89038)]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Flattening list\n",
    "final_ordered_list_with_index = list(itertools.chain(*buckets_results))\n",
    "\n",
    "print(\n",
    "    f'Primeiros 3 elementos da lista:\\n{final_ordered_list_with_index[:3]}\\n\\n'\n",
    "    f'Útimos 3 elementos da lista:\\n{final_ordered_list_with_index[-3:]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Podemos verificar que a estratégia funcionou. Como resultado final podemos obter, nos testes realizados, um ganho de 6 segundos (de 26s para 20s) da versão executada em núcleo um para a versão executada em todos. Claro que nesse tempo também há o overhead da geração gerenciamento das tarefas, além da união dos resultados no final. Porém a tendência é o ganho de desempenho se tornaria cada vez maior, a medida que a massa de dados fosse aumentada. Como saída do processo, temos um array de tuplas ordenado, a primeira posição indicando a quantidade de ocorrências e a segunda o índice do registro no data frame original.\n",
    "\n",
    "### Execuções no Spark\n",
    "\n",
    "[Execução em um núcleo](https://raw.githubusercontent.com/ViniciusBernardo/lista3-ViniciusFerreira-IcaroPires/master/execucao_1_nucleo.jpg)\n",
    "\n",
    "[Execução em 4 núcleos](https://raw.githubusercontent.com/ViniciusBernardo/lista3-ViniciusFerreira-IcaroPires/master/execucao_4_nucleos.jpg)\n",
    "\n",
    "## Links externos\n",
    "\n",
    "[Plataforma de Dados Abertos](http://dados.gov.br/)\n",
    "\n",
    "[Dados de Ocorrências Policiais](http://dados.gov.br/dataset/sistema-nacional-de-estatisticas-de-seguranca-publica)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
